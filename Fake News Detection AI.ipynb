{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import heapq\n",
    "from IPython.display import clear_output\n",
    "\n",
    "import nltk\n",
    "\n",
    "# import lemmetizer modules\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# get set of stopwords\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    House Dem Aide: We Didn’t Even See Comey’s Let...\n",
       "1    Ever get the feeling your life circles the rou...\n",
       "2    Why the Truth Might Get You Fired October 29, ...\n",
       "3    Videos 15 Civilians Killed In Single US Airstr...\n",
       "4    Print \\nAn Iranian woman has been sentenced to...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing train dataset into a dataframe\n",
    "\n",
    "trainDataset = pd.read_csv(\"Dataset/train.csv\")[\"text\"]\n",
    "trainDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    PALO ALTO, Calif.  —   After years of scorning...\n",
       "1    Russian warships ready to strike terrorists ne...\n",
       "2    Videos #NoDAPL: Native American Leaders Vow to...\n",
       "3    If at first you don’t succeed, try a different...\n",
       "4    42 mins ago 1 Views 0 Comments 0 Likes 'For th...\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# importing test dataset into a dataframe\n",
    "\n",
    "testDataset = pd.read_csv(\"Dataset/test.csv\")[\"text\"]\n",
    "testDataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BOW:\n",
    "    # function to extract unnecessary word from sentence\n",
    "    def extractWord(self, word):\n",
    "        ignore = set(stopwords.words('english'))\n",
    "        words = re.sub(\"[^\\w]\", \" \", word).split()\n",
    "        words = ''.join(words).lower()\n",
    "        \n",
    "        if words in ignore:\n",
    "            return ''\n",
    "        \n",
    "        return words\n",
    "    \n",
    "    # function to tokenize all the sentences in the dataset\n",
    "    def tokenize(self, sentence):\n",
    "        words = []\n",
    "\n",
    "        sentence = sentence.split()\n",
    "        for word in sentence:\n",
    "            w = lemmatizer.lemmatize(self.extractWord(word))\n",
    "\n",
    "            if w != '':\n",
    "                words.append(w)\n",
    "\n",
    "        return words\n",
    "    \n",
    "    # create bag of words\n",
    "    def createBOW(self, dataframe):\n",
    "        sentences = dataframe.astype(str)\n",
    "        bow = [{} for _ in range(len(sorted(list(set(sentences)))))]\n",
    "\n",
    "        for i, sentence in enumerate(sentences):\n",
    "            words = sorted(self.tokenize(sentence))\n",
    "            \n",
    "            for word in words:\n",
    "                if word not in bow[i].keys():\n",
    "                    bow[i][word] = 1\n",
    "                else:\n",
    "                    bow[i][word] += 1\n",
    "                    \n",
    "            clear_output(wait=True)\n",
    "            print(f'Creating Bag Of Words: {i + 1}/{len(bow)}')\n",
    "                    \n",
    "        return bow\n",
    "    \n",
    "    def __init__(self, dataset):\n",
    "        self.data = self.createBOW(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Bag Of Words: 5/5\n"
     ]
    }
   ],
   "source": [
    "trainBOW = BOW(trainDataset)\n",
    "testBOW = BOW(testDataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainBOW = pd.DataFrame(trainBOW.data)\n",
    "testBOW = pd.DataFrame(testBOW.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
